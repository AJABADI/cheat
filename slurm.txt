# Misc
/usr/local/Cluster-Docs/SLURM/slurm_submit.darwin slurm_submit
1 gpu hour = 6 cpu hours
  practice: hours * 10

# Account
mybalance   // how many hours are left

# Module
list  // currently loaded
avail // available
load name // load module
unload name // unload module

# sbatch
SBATCH_ACCOUNT
SBATCH_JOB_NAME
SBATCH_QOS

sbatch slurm_submit
  -J jobname
  -D workdir
  -o stdout.log (default: slurm*)
  -e stderr.log (default: stdout)
    using only -o: writes first stderr, and appends stdout at end of job
  --ntasks // number of processes per node
  --nodes min[-max] // number of nodes
    => total # processes = # processes per node * # nodes
  --mem mem // memory in MB per node
  --mem-per-cpu mem // minimum memory in MB per CPU
  --mincpus n  // min # cpus per node
  --test-only
  --qos=INTR  // for fast running jobs
  --profile=[all|task|network]
  -A STEGLE-SL2-GPU // acccount

## profiling
/usr/local/Cluster-Config/slurm/profile_data/ca426
sh5util


# Others
squeue
  -u ca426
  -p [tesla|sandybridge]
scontrol show job <jobid> // sshow
scancel/skill <jobid>
  -u ca426  // all jobs
  -n jobname // by name


# Disk quota
quota
home: 40 GB, backup
scratch: unlimited, no backup
