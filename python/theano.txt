# misc
theano.test() // test theano installation

# configurations
theano.configs
print(theano.configs)
## precedence
1. theano.config.floatX = 'float32'
2. THEANO_FLAGS='floatX=float32, ...'
3. .theanorc
[global]
  floatX = float32
  device = gpu0


print(theano.configs) // show all configurations

# compilation failure error on MacOS
add to ~/.theanorc:
[gcc]
cxxflags=-march=corei7



# printing
from theano import pp
a = dscalar('a')
b = dscalar('b')
c = a + b
pp(c) // nice formatting c
theano.printing.pprint(c) // same as pp
theano.printing.debugprint(c) // debug graph
theano.printing.pydotprint(c, ...) // draw graph with pydot
  outfile='file.png'
from IPython.display import Image
Image('file.png', width='100%') // draw in notebook



# tensor
import theano.tensor as T

## data type
* http://deeplearning.net/software/theano/library/tensor/basic.html
dscalar, dvector, dmatrix, drow, dcol
dscalar() // double no name
dscalar('a')  // double named a
a, b = dscalars('a', 'b') // multiple scalars
scalar()  // float64
scalar('a', dtype='float32')  // a float32
vector()  // 1d: [1, 2, 3]; not 2d [[1, 2, 3]], [[1], [2], [3]]
matrix()  // 2d: [[1, 2], [3, 4]]
col() // 2d column vector: [[1], [2], [3]]
row() // 2d row vector: [[1, 2, 3]]
* bool not supported!
bX  // int8
wX  // int16
iX  // int32
lX  // int64
fX  // float32
dX  // double64
b = T.cast(a, 'float32')  // change dtype (np.float32 not allowed)

## attributes
tensor.shape

## creation
y = T.ones_like(x)  // same shape as old; filled with ones
zeros_like(x)
fill(x, value)
concatenate((x1, x2), axis=0)
eye(n)  // diagonal

## operations
arange(scalar)  // create vector of length scalar
sum, prod, min, max, maximum, argmax
exp, log, transpose
eq, neq, gt, lt // ==, !=, >, <
nnet.softmax  // softwax row-wise (columns are classes)
nnet.binary_crossentropy(z, y) // -y log(z) - (1-y)log(1-z)
  * y=[0, 1], z=[0.1, 0.9]
  * returns vector, if y and z are vectors
  * returns inf if z=0
nnet.categorical_crossentropy(y, z) // -sum_i y_i * log(z_i);  // not mean!
  * x matrix; each row prob distribution
  * y matrix: each row prob distribution
  * y vector: y[i] is true class
T.dot(matrix, vector) -> vector // no difference row/column vector
T.dot(matrix, col_matrix) -> col_matrix
T.dot(row_matrix, matrix) -> row_matrix
T.flatten(matrix) -> vector // like numpy.flatten()
T.reshape(dim)  // like numpy.reshape


## computing gradients / derivatives
expr = x**2 + y
gx, gy = T.grad(expr, [x, y]) // expr wrt. x and y
f = function([x], gx)
f(1.0)  // evaluate gradient




# functions
f = theano.function([input1, input2], output, ...)
  givens={x:value_x, y:value_y} // substitute variables is graph
  allow_input_downcast=True // allow downcast of input arguments; e.g. int64 -> int32
  mode='FAST_COMPILE' // compile fast; python for evaluation
  mode='FAST_RUN' // compile slow; C for evaluation
  mode='DebugMode'  // debugging
  update=[(shared, new_shared), ...]  // only shared; order is ignored
f(input1, input2) // evaluate function
output.eval({input1: value1, input2: value2}) // alternative way to evaluate; not as flexible as function
function([x], [y1, y2, y3]) // multiple output arguments
function([x1, theano.Param(x2, default=1)], y)  // default argument




# shared variable
* stores data value that is shared in the compilation graph
s = theano.shared(value=init_value, ...)
  name=
  borrow=False  // default: copy value into shared variable
  borrow=True  // store reference of value; if value is big; not on GPU

s.get_value(borrow=False) // return COPY, if borrow=False (default)
s.set_value(x, borrow=False)  // set COPY, if borrow=False (default)
s.type  // scalar, matrix, ... -> inferred from value
shared(1).type // scalar
shared(ones(3)).type // vector
shared(ones((3, 1))).type // matrix
shared(ones((1, 3))).type // matrix
shared(other.get_value(borrow=True))  // copy shared variable
function(updates=[(s, s + 1), (r, f(r)])  // update shared r by f(r)





# random numbers
* symbolic variables in compilation graph
* value is drawn on execution when needed
* http://deeplearning.net/software/theano/library/tensor/raw_random.html#libdoc-tensor-raw-random
from theano.tensor.shared_randomstreams import RandomStreams
rs = RandomStreams(seed=0)
rs.seed(0)  // set shared seed for all rng r
rs.uniform(size=(nrow=, ncol), low=0, hight=1)
rs.normal(size=(nrow, ncol), avg=0.0, std=1.0)
rs.binomial(size=(nrow, ncol), n=1, p=0.5)


# debugging / compilation graph
a = T.dscalar('a')
b = T.dscalar('b')
c = a + b
c.owner.inputs  // a, b
c.owner.op.name // operation name

# scan, looping
results, updates = th.scan(fun, ...)
  fun=lambda prev, non_sequence: // function in loop
  non_sequence= // constant in loop
  output_info=  // initial value
  n_steps=  // # interactions
results = [result0, result1, ...] // result of all iterations

# missing values
* http://deeplearning.net/software/theano/library/tensor/basic.html#indexing
Y has missing values
M is 1/0 integer matrix (theano supports not bool and boolean indexing)
Y[M.nonzero()]
  

