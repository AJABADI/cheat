# misc
theano.test() // test theano installation


# installation
## ebi
requires libpython3.4.so
compile python with --enable-shared
./configure.sh --enable-shared --prefix=$STOW/python3.4.2

## intel mkl
[blas]
ldflags = -L/nfs/research2/stegle/software/opt/intel/composer_xe_2013.2.146/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lpthread -lm

# compilation failure error on MacOS
add to ~/.theanorc:
[gcc]
cxxflags=-march=corei7

# configurations
theano.configs
print(theano.configs) // show all configs
## precedence
1. theano.config.floatX = 'float32'
2. THEANO_FLAGS='floatX=float32,device=gpu'
3. .theanorc
[global]
  floatX = float32
  device = gpu

# Nodes
a = tht.scalar('a')
b = tht.scalar('b')
c = a + b

ap = c.owner
  * owner is not None if c is output of operation
type(ap) // theano.gof.graph.Apply
assert ap.inputs[0] is a
assert ap.inputs[1] is b
assert ap.outputs[0] is c
ap.op // <theano.tensor.elemwise.Elemwise at 0x10bdc80d0>
  * defines operation on input
  * ap.op == function declaration (def fun(x, y): ...)
  * ap == function result (fun(x, y))
type(a) // theano.tensor.var.TensorVariable
type(c) // theano.tensor.var.TensorVariable
a.type // TensorType(int32, scalar)
b.type // TensorType(float32, scalar)
  * type defines set of constraints (dtype, dim, etc.)


# OpFromGraph
* Define operation(x, y, z) of tensor variables
* function: operations of numpy variabiles
## Example
x, y, z = tht.scalars('xyz')
e = x + y * z
o = th.OpFromGraph([x, y, z], [e])
e2 = o(x, y, z) + o(z, y, y)
f = th.function([x, y, z], [e2])

# scan, looping
* recursion on tensor variable
results, update = th.scan(fun, ...)
  sequences=x // x[0], x[1], x[2], ..., passed to fun each time step
  outputs_info= // initial value result of fun
  n_steps=n  // # recursions, n_steps == len(x
  non_sequences=const // constants in operation
results = [result0, result1, ...] // result of all iterations
fun([sequences], [last_output_fun], [non_sequences])
  * args are only passed, if specified as sequences=, outputs_info=, non_sequences=)
## Example: x^n
n = tht.iscalar('n')
x = tht.scalar('x')
xns, update = th.scan(lambda xl, x: xl * x,
                      outputs_info=tht.ones_like(x),
                      non_sequences=x,
                      n_steps=n)
xn = xns[-1]
f = th.function([x, n], xn)


# printing
from theano import pp
a = dscalar('a')
b = dscalar('b')
c = a + b
pp(c) // nice formatting c
theano.printing.pprint(c) // same as pp
theano.printing.debugprint(c) // debug graph
theano.printing.pydotprint(c, ...) // draw graph with pydot
  outfile='file.png'
from IPython.display import Image
Image('file.png', width='100%') // draw in notebook



# tensor
import theano.tensor as T

## data type
* http://deeplearning.net/software/theano/library/tensor/basic.html
dscalar, dvector, dmatrix, drow, dcol
dscalar() // double no name
dscalar('a')  // double named a
a, b = dscalars('a', 'b') // multiple scalars
scalar()  // float64
scalar('a', dtype='float32')  // a float32
vector()  // 1d: [1, 2, 3]; not 2d [[1, 2, 3]], [[1], [2], [3]]
matrix()  // 2d: [[1, 2], [3, 4]]
col() // 2d column vector: [[1], [2], [3]]
row() // 2d row vector: [[1, 2, 3]]
* bool not supported!
X // config.floatX is default dtype
bX  // int8
wX  // int16
iX  // int32
lX  // int64
fX  // float32
dX  // float64
b = T.cast(a, 'float32')  // change dtype (np.float32 not allowed)

## attributes
tensor.shape

## creation
y = T.ones_like(x)  // same shape as old; filled with ones
zeros_like(x)
fill(x, value)
concatenate((x1, x2), axis=0)
eye(n)  // diagonal

## operations
arange(scalar)  // create vector of length scalar
sum, prod, min, max, maximum, argmax
exp, log, transpose
eq, neq, gt, lt // ==, !=, >, <
nnet.softmax  // softwax row-wise (columns are classes)
nnet.binary_crossentropy(z, y) // -y log(z) - (1-y)log(1-z)
  * y=[0, 1], z=[0.1, 0.9]
  * returns vector, if y and z are vectors
  * returns inf if z=0
nnet.categorical_crossentropy(y, z) // -sum_i y_i * log(z_i);  // not mean!
  * x matrix; each row prob distribution
  * y matrix: each row prob distribution
  * y vector: y[i] is true class
T.dot(matrix, vector) -> vector // no difference row/column vector
T.dot(matrix, col_matrix) -> col_matrix
T.dot(row_matrix, matrix) -> row_matrix
T.flatten(matrix) -> vector // like numpy.flatten()
T.reshape(dim)  // like numpy.reshape


## computing gradients / derivatives
expr = x**2 + y
gx, gy = T.grad(expr, [x, y]) // expr wrt. x and y
f = function([x], gx)
f(1.0)  // evaluate gradient




# functions
f = theano.function([input1, input2], output, ...)
  givens={x:value_x, y:value_y} // substitute variables is graph
  allow_input_downcast=True // allow downcast of input arguments; e.g. int64 -> int32
  mode='FAST_COMPILE' // compile fast; python for evaluation
  mode='FAST_RUN' // compile slow; C for evaluation
  mode='DebugMode'  // debugging
  update=[(shared, new_shared), ...]  // only shared; order is ignored
f(input1, input2) // evaluate function
output.eval({input1: value1, input2: value2}) // alternative way to evaluate; not as flexible as function
function([x], [y1, y2, y3]) // multiple output arguments
function([x1, theano.Param(x2, default=1)], y)  // default argument




# shared variable
* stores data value that is shared in the compilation graph
s = theano.shared(value=init_value, ...)
  name=
  borrow=False  // default: copy value into shared variable
  borrow=True  // store reference of value; if value is big; not on GPU

s.get_value(borrow=False) // return COPY, if borrow=False (default)
s.set_value(x, borrow=False)  // set COPY, if borrow=False (default)
s.type  // scalar, matrix, ... -> inferred from value
shared(1).type // scalar
shared(ones(3)).type // vector
shared(ones((3, 1))).type // matrix
shared(ones((1, 3))).type // matrix
shared(other.get_value(borrow=True))  // copy shared variable
function(updates=[(s, s + 1), (r, f(r)])  // update shared r by f(r)





# random numbers
* symbolic variables in compilation graph
* value is drawn on execution when needed
* http://deeplearning.net/software/theano/library/tensor/raw_random.html#libdoc-tensor-raw-random
from theano.tensor.shared_randomstreams import RandomStreams
rs = RandomStreams(seed=0)
rs.seed(0)  // set shared seed for all rng r
rs.uniform(size=(nrow=, ncol), low=0, hight=1)
rs.normal(size=(nrow, ncol), avg=0.0, std=1.0)
rs.binomial(size=(nrow, ncol), n=1, p=0.5)


# debugging / compilation graph
a = T.dscalar('a')
b = T.dscalar('b')
c = a + b
c.owner.inputs  // a, b
c.owner.op.name // operation name

# missing values
* http://deeplearning.net/software/theano/library/tensor/basic.html#indexing
Y has missing values
M is 1/0 integer matrix (theano supports not bool and boolean indexing)
Y[M.nonzero()]
  

