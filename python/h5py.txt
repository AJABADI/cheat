# General
* http://docs.h5py.org/en/latest/quick.html
* Optimized for numpy
* pytables (tables) is more abstract (complicated)
* Parallel read ok; parallel write will corrupt file!

# Notation
f: File
g: group
[fg]: File or group

# Write numpy array
f = File('file.h5', 'w')  // use 'a' to append, else overwrite
if '/g1/d1' in f:
  del f['/g1/d1']
[fg].create_dataset('/g1/d1', data=array)
fg['/g1/d1'] = array
f.close()

# Groups
g = f.create_group('g1')
g['dataset'] = array

# Links
f['alias'] = d['dataset'] // internal link
f['alias'] = h5.ExternalLink('file.h5', '/g1/d1') // external link


# Overwrite dataset
if 'd' in f:
  def f['d']
d.create_dataset('d', ...)

# Read dataset
d = [fg]['dataset']
d.value
with d.astype('float32'):
  v = d.value // conversion done by h5py

# Slicing data
d = f['dataset']
d.value // everything
d[...]  // everything
d[i]  // at index i
d[[i, j, k]]  // at indices (increasing order)
d[:k] // every kth
d[[True, False, ...]] // via boolean


# Explore file
f = File('file.h5', 'r')
f.items() // items as dict
for k, v in f.items():
  data[k] = v.value // v if HDF5 dataset object
list(f.keys())  // list keys
g = f['/group']
v = g['/item'].value  // read data
g.name  // path
g.keys()  // children
g.get('child')

# open attributes
* r: read-only
* w: overwrite existing file (all groups)
* r+ : read/write; file must exist
* a : read/write; file does not have to exist

# edit file
f = File('file.h5', 'r+')
d = f['dataset']
d[0, 0] = 10
f.close()

# Create_dataset
d = [fg].create_dataset(path, ...)
  shape=(n, m)
  dtype='float32'
  data=data // initialize with data
  compression='gzip'
  compression_opts=4  // compression level, default=4
  maxshape=(None, 100)  // resizeable dataset

## Examples
d[:i, :j] = value // Assign value to parts
f = [fg].create_dataset('d1', data=d1)   // initialize with data
d = [fg].create_dataset('d1', (n,m), dtype='i') // initialize empty
d = f.create_dataset('d1', (10,), dtype=('i4, f4, a10'))  // structured array
  d[0] = (1, 1.2, 'Christof')

## Chunking
create_dataset((1e5, 1e5), chunks=(100, 100))
* split dataset into chunks of size (100, 100)
* access by B-tree instead of linear access
* keep chunk size between 10 KB and 1MB
* not everything is loaded into memory


# delete dataset
del f['/group/dataset']

# modify dataset
d = f['/data']
d[...] = new_value

# check if dataset exists
f = File(...)
'/data' in f  // True or False
'/group/data' in f  // True or False

# Attributes
attr = [fg].attr
list(attr.keys) // list attributes
attr['a']  // get value
attr['a'] = 'str'
attr['a'] = 10
attr['a'] = np.arange(10)

# Storing strings
* need to be byte-encode
s = [x.encode() for x in ['a', 'b', 'c']
[fg].create_dataset(data=s)
[fg].attrs['columns'] = s
