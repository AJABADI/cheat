# General
* http://docs.h5py.org/en/latest/quick.html
* Optimized for numpy
* pytables (tables) is more abstract (complicated)
* Parallel read ok; parallel write will corrupt file!

# Notation
f: File
g: group
[fg]: File or group

Indexing issues
===============
* Boolean mask only works if same shape as dset
  dset[[True, True, False]] fails, if dset.ndim > 1
  -> convert mask to indices with mask.nonzero()[0]
* Indices for singe mask fails
  dset[[0, 1]] fails, if dset.ndim > 1
  dset[[0, 1], :] works, for dset.ndim == 2

# Write numpy array
f = File('file.h5', 'w')  // use 'a' to append, else overwrite
if '/g1/d1' in f:
  del f['/g1/d1']
[fg].create_dataset('/g1/d1', data=array)
fg['/g1/d1'] = array
f.close()

# Groups
g = f.create_group('g1')
g['dataset'] = array

# Links
f['alias'] = d['dataset'] // internal link
f['alias'] = h5.ExternalLink('file.h5', '/g1/d1') // external link


# Overwrite dataset
if 'd' in f:
  def f['d']
d.create_dataset('d', ...)

# Read dataset
d = [fg]['dataset']
d.value
with d.astype('float32'):
  v = d.value // conversion done by h5py

# Slicing data
d = f['dataset']
d.value // everything
d[...]  // everything
d[i]  // at index i
d[[i, j, k]]  // at indices (increasing order)
d[:k] // every kth
d[[True, False, ...]] // via boolean


# Explore file
f = File('file.h5', 'r')
f.items() // items as dict
for k, v in f.items():
  data[k] = v.value // v if HDF5 dataset object
list(f.keys())  // list keys
g = f['/group']
v = g['/item'].value  // read data
g.name  // path
g.keys()  // children
g.get('child')

# open attributes
* r: read-only
* w: overwrite existing file (all groups)
* r+ : read/write; file must exist
* a : read/write; file does not have to exist

# edit file
f = File('file.h5', 'r+')
d = f['dataset']
d[0, 0] = 10
f.close()

# Create_dataset
d = [fg].create_dataset(path, ...)
  shape=(n, m)
  dtype='float32'
  data=data // initialize with data
  compression='gzip'
  compression_opts=4  // compression level, default=4
  maxshape=(None, 100)  // resizeable dataset


## Examples
d[:i, :j] = value // Assign value to parts
f = [fg].create_dataset('d1', data=d1)   // initialize with data
d = [fg].create_dataset('d1', (n,m), dtype='i') // initialize empty
d = f.create_dataset('d1', (10,), dtype=('i4, f4, a10'))  // structured array
d[0] = (1, 1.2, 'Christof')

[fg].create_dataset(...)
  compression='gzip'
  compression_opts=4  // compression level, default=4
  maxshape=(None, 100)  // resizeable dataset

## Chunking
* Default: data stored contiguously
* chunks=(n, m): store data as chunks in random order on disk
* if data element is accessed, whole chunk is read into memory
* required for resizing elememnts
chunks=None // no chunking by default
chunks=True // auto chunking
chunks=(n,m) // chunk size n x m; tuple, not list or int required !!

## Resizeable datasets
ds = f.create_dataset(name, shape=(nb_row,...), maxshape=(nb_max,...)
  * maximum nb_row rows at beginning, can later be increased with resize(...)
  * size is not resized automatically
  * maxshape is max that can be used in resize(...)
ds.resize((nb_new_row, ...)) // increase or decreases size


# delete dataset
del f['/group/dataset']

# modify dataset
d = f['/data']
d[...] = new_value

# check if dataset exists
f = File(...)
'/data' in f  // True or False
'/group/data' in f  // True or False

# Attributes
attr = [fg].attr
list(attr.keys) // list attributes
attr['a']  // get value
attr['a'] = 'str'
attr['a'] = 10
attr['a'] = np.arange(10)

# Storing strings
* need to be byte-encode, unicode not supported!
* dtype='S8' -> string of length 8
s = [x.encode() for x in ['a', 'b', 'c']]
for idx, value in np.ndenumerate(s):
  s[idx] = value.encode()
[fg].create_dataset(data=s, dtype=s.dtype)
[fg].attrs['columns'] = s

# Bug: large file
if group in out_file:
  del out_file[group]

# Copy groups between files
for k in src_file.keys():
  src_file.copy(k, dst_file)
