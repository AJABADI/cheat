# Data
import sklearn.datasets
load_iris
load_digits
fetch_mldata  // download digits from ML repository
.data // design matrix
.target // classes as integers
.target_names

# Hyperparameter optimization
rf = RandomForestClassifier()
params = {'max_depth': [4, 8, 12], 'n_estimator': [8, 12]}

## GridSearchCV
from sklearn.grid_search
m = GridSearchCV(rf, params, ...)
  cv=x  // CV method --> see below
    Number of folds
    PredefinedFold(), StratifiedKFold(), ...
  scoring='accuracy, roc_auc' // http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
  n_jobs=1
m.fit(X, y) // Split into train/val according to cv=
m.best_params_
m.best_estimator_
m.grid_scores_ // scores and parameter settings

## RandomizedSearchCV
params = {
  'max_depth': scipy.stats.randint(4, 12), // define distribution
  'n_estimator': [0, 1] // draw from array
m = RandomizedSearchCV(rf, params, ...)
  n_iter=10


## ParameterSampler
* Generators for parameters
p = {'a': sps.uniform(0, 0.5), 'b': sps.randint(0, 10), 'c': ['x', 'y', 'z']}
p = ParameterSampler(p, 10)
for x in p:
list(p)

## Predefined fold
x = PredefinedFold([0, 0, 1, 1])
  * Train on 0, val on 1 and VICE VERSA!
x = PredefinedFold([-1, -1, 0, 0])
  * Train on 0, val on 0 without VICE VERSA!


# PCA
import sklearn.decomposition
pca = PCA(n_components=2, white=False)
pca.fit(Y)
  Y = X W // n x p
  X = [-] // samples in rows; n x q
  W = [-] // eigenvectors in rows; q x p
X = pca.fit_transform(Y)
pca.components_  // W
pca.transform(Y)  // X; n x q
pca.inverse_transform(X)  // Y
pca.explained_variance_

# metrices
import sklearn.metrices
accuracy_score(true, pred)  // accuracy
precision_score(true, pred) // precision
recall_score(true, pred)  // TPR
roc_auc_score(true, pred)
classification_report(true, pred)
c = skm.confusion_matrix(y, z.round())
  tpr = c[1, 1] / c[1].sum()
  fpr = c[0, 0] / c[0].sum()
matthews_corrcoef(y, z.round())


## curves
fpr, tpr, thr = roc_auc_curve(true, score)
pre, recall, thr = precision_recall_curve(true, score)


## confusion matrix
confusion_matrix(true, pred)
* rows: true
* columns: predicted

# parameter grid
from sklearn.grid_search import ParameterGrid
p = ParameterGrid({'a': [...], 'b': [...]})
for pp in p:
  model.set_params(**pp)






# preprocessing
import sklearn.preprocesing as pp


## Mean normalization
s = StandardScaler(copy=True, with_mean=True, with_std=True)
  copy=False  // don't copy data on transform, fit_transform
s.fit(X)
  s.mean_ // column means
  s.std_ // column std
Z = s.transform(X)
Z = s.fit_transform(X)
X = StandardScaler(copy=False).fit_transform(X)

## MinMax normalization
s = MinMaxScaler(feature_range=(0, 1))
  * scales all features independently between (0, 1)
  * same interface as StandardScaler
s.fit(X)
  x.min_ is not X.min(axis=0) !!

## pp.LabelBinarizer
* labels -> binary
fit(['a', 'a', 'b'])
transform(['a', 'b', 'b']) -> [[0], [1], [1]]

## pp.LabelEncoder
* labels -> integer
fit(['a', 'a', 'b'])
transform(['a', 'b', 'b']) -> [0, 1, 1]

## pp.OneHotEncoder
* integers -> binary
enc = OneHotEncoder(...)
  n_values='auto'  // # values per features -> # bits
    ! Set manually !
  sparse=True // return sparse matrix
fit([[1, 2], [2, 3], [2, 4]]) // rows = samples, columns = features
transform([[1, 2]]) // transform features of single sample to sparse binary mat
  .todense()  // dense matrix
feature_indices_[i] // start col of bits in binary matrix; fi[i]:fi[i+1] is code
### Encoding
0 -> 1 0 0
1 -> 0 1 0
2 -> 0 0 1



# Validation

## Hold-out split
import sklearn.cross_validation as cv
train_X, test_X, train_y, test_y = cv.train_test_split(X, y, train_size=0.6)
  * inputs are *args, not kwargs!


## Sampling
StratifiedShuffleSplit(labels, n_iter, test_size=0.5) // stratified bootstrapping
Bootstrap(num_samples, test_size=0.5) // unstratified bootstrapping






# decision tree
import sklearn.tree.DecisionTreeClassifier
import sklearn.tree
tree.export_graphviz(dt, 'file.dot')  // export as *.dot -> viz with graphviz

# contributing
* https://github.com/scikit-learn/scikit-learn/pull/4566#issuecomment-93368443
git checkout master
git pull upstream master
git checkout feature-branch
git rebase master

# copy / clone model
clone = sklearn.base.clone(model)

