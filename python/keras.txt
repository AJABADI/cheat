 Misc
######
K._BACKEND // tf / th
K.floatx()  // default float precision
KERAS_BACKEND="tensorflow|theano" // env backend variable

Reinitializing weights
======================
w0 = model.get_weights()
model.set_weights(w0)

Getting shape of tensors and layers
===================================
* tensor: x.get_shape()  // only for TensorFlow
* layer: l.output_shape() // after it has been called with xx = l(x)



Dependencies
############
       model.py       topology.py
----------------------------------
Sequential -> Model -> Container
              Dense ->  Layer

Container
#########
c = Container(inputs, outputs, name=None)

Attributes
==========
inputs // list of input tensors
input_names // names of input tensors
input_shape // shape of input tensors
outputs // list of output tensors
output_names // names of output tensors

Methods
=======
get_weights()
summary()
layers() // list of layers
get_layer(name) // layer by name
output_layers() // last layers
layers_by_depths[i] // ith layer


Model(Container)
################

Attributes
==========
optimizer
  .lr.set_value(0.01)
  lr.get_value()
stop_training // True if early stopping

Methods
=======
m = Model(optimizer, loss)
m = Model(inputs=, outputs=)
compile(
  loss_weights={'out1': 2.0}
    * list or dict
    * missing outputs will get weight 1.0
    * are not normalized: weight 100 -> loss * 100
  )

Saving model
============
model.save(path) // configs and weights to HDF5
  -> keras.models.load_model(path)
model.save_weights(path)
  -> model.load_weights(path)
model.get_weights() // weights as numpy arry
model.get_layer('name').get_weights() // weights of layer
model.to_json(), keras.model
  -> keras.models.model_from_json(json)
model.to_yaml()


Layers
######

Functional API
==============
x = kl.Input(shape=(512,))
y = kl.Dense(...)(x)
m = Model(input=[x], output=[y])

Attributes
==========
name
input / output
trainable

Methods
=======
set_weights(weights)
get_weights()
get_config()
call(x)
count_params()
summary() // short model summary


Core
====
Reshape((3, 4)) // reshapes layer with 3*4 features to (3, 4)
Permute((2, 1)) // changes dimensions (n, 3, 4) to (n, 4, 3)
merge((l1, l2), mode='concat', concat_axis=1, ...) // lerges layers
* Same as K.concatenate((l1, l1), axis=1)
Merge(...) // merges models


Dense
-----
core.Dense(nunits, ...)
  activation='relu'
  W_regularizer='l1', regularizer
  init='glorot_uniform', init
  input_dim=x // # previous units; required for first layer
  input_shape=(x,) // same as input_dim, but tuple

Wrapper layers
--------------
* Layers that depend on other layers
* Examples: TimeDistributed, Bidirectional, Attention
l = Wrapper(layer)
l(x)

Lambda layers
-------------
* Helper function to write simple layers
* l = Lambda(lambda x: x**2)



Convolution
===========
convolutional.Convolution2D(nkernels, ninput_channels, m, n, ...)
  * input: samples x channels x rows x columns
  * output: samples x nkernels x rows' x columns'
  border_mode='valid,full,same,valid'
    full: n + k - 1 // zero-padding
    same: n // zero-padding, preserve size  -> prefer
    valid: n - k + 1  // no zero-padding
  activation='linear'
  init='glorot_uniform'
  W_regularizer=None
  subsample=(1,1) // step/stride size, 2 -> take every 2nd

convolutional.Convolution1D(input_dim, nb_filters, input_len)
  * input: samples x seq_len x input_dim
    input_dim is nb_filters/channels of previous layer
  * output: samples x seq_len' x nb_filters
  * difference 2D: slides filter only vertically, not horizontally
  subsample_length=1 // step/stride size
convolutional.MaxPooling2D(...)
  poolsize=(2,2)

Visualizing filters
===================
filters = m.layers[i].get_weights()[0]  // get filters
  num_filters x input_dim x filter_len x 1  // Convolution1D

border modes
============
scipy.signal.convolve(x, k, mode='full')
* convolution over x with kernel k
>> full
* size: n + k - 1
k k k             k k k
    x x x  -> x x x
>> same
* size: n
k k k         k k k
  x x x  -> x x x
>> valid
* size: n - k + 1
k k k             k k k
x x x x x  -> x x x x x


Embedding layers
================
embedding(input_dim, output_dim)
* Transforms nsamples x seq_len --> nsamples x seq_len x output_dim
* input_dim is maximum integer (word index) in input vector
* Y = X W, where X is one-hot encoding of words


Recurrent layers
================
input shape: nsamples x ntimesteps x input_dim
output shape:
  nsamples x output_dim if return_sequences == False
  nsamples x ntimesteps x output_dim if return_sequences == True
return_sequences=False // only return last hidden state
truncate_gradient=-1 // only compute gradient over last x steps

LSTM
----
activation='tanh'  // activation h'
inner_activation='hard_sidmoid' // activation gates

GRU
---
activation='sigmoid'  // activation h'
inner_activation='orthogonal' // activation gates



Callbacks
=========
import keras.callbacks
CallBack  // base class
EarlyStopping
ModelCheckpoint   // save model weights to file
LearningRateScheduler



Utils
#####
import keras.utils

data_utils
==========
get_file(fname, url) // downloads file into cache directory

np_utils
========
to_categorical([0, 1, 2], 3)  --> [[0 0 0] [0 1 0] [1 0 0]]
probas_to_classes() // reverse to_categorical
convert_kernel(kernel) // converts single kernel from TF <-> Theano
layer_utils.convert_all_kernels_in_model(model) // converts all kernels

io_utils
========
HDF5Matrix(path, dataset, start, end) // View on subset of matrix




Applications
############
* Builds standard model, which can be initialized with pretrained weights
* VGG16, VGG19, Inception, Resnet

from keras.applications.inception import InceptionV3
model = InceptionV3(weights='imagenet') // Usual model
  weights=None  // initialize randomly
  weights='imagenet' // use pretrained imagenet weights



Data
####
* stored in ~/.keras/datasets
from keras.datasets import (mnist, cifar10, imdb, ...)
(X_train, y_train), (X_test, y_test) = data.load_data()

* mnist: digit classification
* cifar10: small image classification
* cifar100: large image classification
* imdb: text sentiment classification
  X_ij = rank frequency word j in movie description i
  y_i = sentiment movie (0=positive, 1=negative
* reuters: text topic classification
  X_ij = rank frequency word j in description i
  y_i = topic 0-45



Backend
#######
dot(x, y) // matrix multiplication
