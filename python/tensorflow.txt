# Misc
tf.reset_default_graph()  // clears everything
tf.py_func(fun, tensor) // wrap Python function
tf.truediv(x, y) // division
tf.minimum(x, y) // element-wise minimum; see also reduce_min()
tf.reduce_min(t, axis)
tf.reduce_max(t, axis)
tf.maximum(x, y) // element-wise maximum; see also reduce_min()
tf.train.exponential_decay(var, step, steps_per_decay, decay_rate, ...)
 * var = var * decay, every steps_per_decay
  staircase=True // decay as discrete intervals

# Creating tenors
tf.ones((3, 4))
tf.ones_like(t) // creates tensor of ones with same shape and dtype as t
tf.zeros((3, 4))
tf.zeros_like(t)
tf.fill((3, 4), value)
tf.constant(value, dtype=tf.float32, shape=(3, 4))
tf.range(min, max)
tf.linspace(min, max)


# Transformations
rf.reshape(x, [-1, 16])
tf.concat(axis, [x1, x2]) // concat tensors
tf.tile(x, [1, 2]) // copy columns twice
tf.expand_dims(x, dim) // insert single dimension/axis at dim in tensor
tf.squeeze(x, squeeze_dims=[]) // squeeze all 1 dim axis, or select axis

# Reductions
tf.reduce_sum(t, axis)
tf.reduce_prod(t, axis)
tf.reduce_mean(t, axis)
tf.reduce_all(t, axis) // logical AND
tf.reduce_any(t, axis) // logical OR
tf.reduce_min(t, axis)
tf.reduce_max(t, axis)

## Conversion
tf.convert_to_tensor(array, dtype='float32') // np.array, list, scalar -> tensor
tensor.eval() // tensor -> array
tf.cast(t, dtype=tf.float32) // change dtype of tensor


# Tensor / Variable
x.get_shape().as_list()
x = tf.Variable(init, ...)
  init = value [1, 2, 3] or Tensor
  trainable=True // set False to exclude from training
sess.run(x) or x.eval(sess) // get value of variable
assign_op = x.assign(value); sess.run(assign_op) // assign, set value variable
x.name // var:0
x.op.name // var
tf.get_variable('var', initializer=tf.constant_initializer(value)) // initialize with value

## Namespace variables
with tf.variable_scope('layer1'):
  w = tf.get_variable('w', shape, dtype='float32',
                      initializer=tf.random_normal_initializer()
                      collections=[tf.GraphKeys, 'filters'],
  b = tf.get_variable('b', shape,
                      initializer=tf.zeros_initializer, // not brackets!
                      collections=[tf.GraphKeys, 'biases']
with tf.variable_scope('layer1', reuse=True):
  tf.get_variable('w').eval(sess) // retrieve value of variable
tf.get_collection_ref('bias') // return all variables from collection

# Collections
* container for variables identified by string
v = tf.get_variable('v', collections=['c1'])
  * adds variable to collection 'c1'
  * creates c1 if non-existing
tf.add_to_collection('c1', tensor) // add any tensor to collection
tf.get_collection('c1') // [v]; returns list of all variables in collection
k = tf.GraphKeys() // default collection names
  .VARIABALES
  .TRAINABLE_VARIABLES
  .GLOBAL_STEP

## scopes
tf.variable_scope(...) // ops + variables
tf.name_scope(...) // ops

# Initializer
tf.constant_initializer(value)
tf.random_uniform_initializer(a, b) // [a, b]
tf.random_normal_initializer(mean, std)
tf.zeros_initializer // no ()
tf.contrib.layers.xavier_initializer



# Retrieve objects
## Variables
for _ in tf.all_variables():
  print(_.name)

## Operations and tensors
for _ in graph.get_operations(): // Operations and placeholders
  print(_.name)
  * 'name:0' will be first output
graph.get_operation_by_name('inception_v3/inception_v3/conv0/Relu') // operation, not output tensor!
graph.get_tensor_by_name('inception_v3/inception_v3/conv0/Relu:0') // output tensor
sess.run('inception_v3/inception_v3/conv0/Relu:0', feed_dict=) // Evaluate tensor by name



# Saving variables
saver = tf.Saver(...)
  {'v1': v1, 'v2': v2} // select variables to store
  max_to_keep=5 // max # checkpoints if global_step is used
saver.save(session, 'model.ckp', ...)
  global_step=0...1 // progress, e.g. epoch
saver.restore(session, 'model.ckp')
tf.train.latest_checkpoint(dir) // get path (string) of latest checkpoint file in dir


# conv2d
tf.nn.conv2d(x, filter, strides, padding)
  x = rf.reshape(x, [-1, height, width, nb_channel])
  x = tf.placeholder('float32', [None, height, width, nb_channel])
  filter = tf.Variable(tf.random.normal([height, width, nb_in-channel, nb_out-channel])
  strides = [1, height, width, 1] // 1 over samples and channels
  padding = 'SAME' (preserve dim by zero-padding), 'VALID' without zero-padding

# pooling
tf.nn.max_pool(x, k=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')

# dropout
rf.nn.dropout(x, keep_prob=0.7) // dropout with 0.3

# Interactive session
sess = tf.InteractiveSession()
operations = tf.initialize_all_variables()
operations.run() // instead of sess.run(operations)
variable.eval() // intead of sess.run(variable)

# control_dependencies
* evaluates op1 and op2 before opx is evaluated
with tf.control_dependencies([op1, op2]):
  opx = other_opp

# tf.ExponentialMovingAverage
ema = tf.ExponentialMovingAverage(decay)

def update_averages():
  _ = ema.apply([mu, sigma]) // op that will eval mu and sigma, and add to avg
  with tf.control_dependencies([_]): // first execute _
    return (tf.identity(mu), tf.identify(sigma)) // return updated mu, sigma


# Tensorboard
## General
tensorboard --logdir path // path containes [train,test_eval,train_eval] dirs
  --logdir m1:./model1/train,m2:model2/train
  --port 6006

tf.scalar_summary(name, tf.reduce_mean(x))
  name=x.op.name + '/mean' // should be x/mean
tf.histogram_summary(name, x)
  name=x.op.name  // should be just x
tf.image_summary('images', x, max_images=3)

summaries = tf.merge_all_summaries() // creates summary op
writer = tf.SummaryWriter('/tmp/tensorflow', sess.graph)

## evaluation
s = sess.run(summaries)
writer.add_summary(s)

# Graphs
g0 = tf.get_default_graph() // returns default graph
g = tf.Graph()
with g.as_default():
  a = tf.Variable(...) // variable of g, not g0
assert a.graph is g
assert a.graph is not g0

with g.as_default():
  sess = tf.Session() // will use g
  sess.run(op_in_g)

tf.Session(graph=graph)
tf.InteractiveSession(graph=graph)


variable.get_graph()
session.get_graph()

# QueueRunners
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess, coord)

# Devices
tf.device(name): // assign to certain device instead of choosing automatically
  /cpu:0 // 1st cpu
  /cpu:1 // 2nd cpu
  /gpu:0 // 1st gpu
  /job:worker/taskX // assign to task X
tf.Session(config=tf.ConfigProto(log_device_placement=True)) // log device placement by run

# IO
1. Feeding from file
sess.run(x, feed_dict={external_data})

2. Reading from file
train.match_filenames_once
train.string_input_producer

filename_queue = train.string_input_producer(['file1.np', 'file2.np'])

# Optimizer
opt = tf.train.AdamOptimizer()
opt.compute_gradients(loss, variables=None)
  * returns [(gradient, variable_name), ...]
  variables=tf.Variable(...)   // wrt. certain tf Variables
opt.apply_gradients(g)


# Transformations
tf.slice(tensor, [x, y], [size_x, size_y]) // slice block at (x, y) of size
tf.gather(tensor, indices)
  indices=[1, 2] // tensor[1], tensor[2] // selects rows
tf.gather_nd(tensor, indices)
  indices=[[x1, y1], [x2, y2]] // selects elements at x1, y1


# Operations
tf.nn.xw_plus_b(x, w, b) // x * w + b

# Image operations
resize_images(image, width, height, ...)
  method=tf.image.ResizeMethod.BILINEAR
crop_center(image, 0.5) // image in 3d; shape of results [?, ?, 3]
pad_to_bounding_box(image, offset_x, offset_y, width, height)
  offset_[xy]: # rows that are filled with 0 left/above
  width, height: size of results image
sample_distorted_bounding_box // samples multiple bbox from single 3d image
extract_glimpse(input, size, offset, ...) // extracts glimpses; fills rest with noise
  input: 4d tensor [batch_size, width, height, channels]
  size: [glimpse_width, glimpse_height]
  offset: 2d tensor [batch_size, x_offset - y_offset]
