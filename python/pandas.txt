# dtypes
df.dtypes // list dtypes
df['c1'] = df['c1'].astype(str) // convert dtype

# Misc
pd.crosstab(y, x) // contingency table
pd.concat((a, b), axis=1) // concat by columns
  keys=['a', 'b'] // create multiindex
pd.merge(a, b)  // merge dataframes by common columns
  on=['c1', 'c2'] // columns to be used for merging
  how='left'  // left join -> fill with np.NaN

# Excluding / selecting columns
df = pd.DataFrame(np.arange(15).reshape(5, 3), columns=list('ABC'))
sel = list('AC')
df_sel = df[sel]
df_unsel = df[[c for c in df.columns if c not in sel]]
* Alternative: df.set_index()

# IO
read_hdf('file', 'groupname')
read_csv(file, ...) / read_table(file, ...)
  nrows // max number of rows to be read
  sep=',' // field separator
  sep='\s+' // separate by spaces
  usecols // select columns by name
  header=None // no header
  names=['a', 'b', 'c'] // column names

# Index
columns // column index
  d['c1']
index // row index
  d.loc['r1']

## Rows
set_index(columns, inplace=False) // use column as index
reset_index(inplace=False) // convert index to columns
sort_index(ascending=True, inplace=False)
reindex(range(1990, 2000))  // adjust to new index
  * drop rows that are not indexed by new index
  * fill with np.NaN for newly indexed rows
  * columns=  // reindex columns
  * index=  // reindex index
  * fill_value=
index.values[0] = 'new_label' // rename index; rename column/row

## Columns
df.columns = ['c1', 'c2'] // rename columns
df.rename(columns={'old':'new'}, inplace=True)  // rename columns
del df['col'] // delete column

## Indexing
df.loc[(r1, r2), c] // row by levels (r1, r2), column c
df.loc[r1, c] // only first level
df.loc[r1].loc[r2] == df.loc[(r1, r2)]

## Creating index
i = Index(array, dtype=)
df = DataFrame(array, index=i)
df.index = i

## Multiindex / Hierarchical index
m = MultiIndex.from_arrays([array1, array2], names=[name1, name2])
m = MultiIndex.from_tuples([('chr1', 1), ('chr1', 2), ])
m = MultiIndex.from_arrays([df.c1, df.c2], names=df.columns[1, 2])  // from DataFrame
df.index = m  // set index
pd.concat((a, b), axis=1, keys=['a', 'b'])  // concat columns of a and b
i = df.columns  // multiindex for columns
i.get_level_values(k) // get column labels level k
i.droplevel(k) // remove level k
df.columns = df.columns.droplevel(k)

## Check if index contains value
value in df.index
'a' in df.index // single-index
'a' in df.index // multi-index first level
('a', 1) in df.index  // mutli-index both levels




# Missing values
np.nan, np.NAN, np.NaN  // pandas compatible
pd.NaT  // Not a Time, for time series
isnull(np.nan) == True
isnull()
notnull()
count() // # non-missing
fillna(0)
dropna(axis=) // drop axis with at least one missing value
interpolate() // fill missing values by interpolation
count(axis=)  // count # non-null elements

# Selection, filtering
s = df.pos  // selection based on series
s > 0 // returns boolean vector
(s > 0) & (s < 5) // boolean operator: brackets; &, |, ~ instead of and, or, not
s[(s > 0) & (s < 5)]  // section series
df[(df.pos > 0) & (df.pos < 5)] // selection data frame
s.isin([3, 5, 5]) // select single values
s.where(s > 5)  // sets entries s <= 5 to NaN
s.where(s < 5, 10)  // sets entries s <= 5 to 10
s.mask(s > 5) // sets entries s > 5 to Nan


# pandas printoptions
pd.set_option('display.width', 150) 
pd.set_option('display.height', 150)
pd.set_option('display.max_columns', 150)
pd.set_option('display.max_rows', 150)

# Melting and reshaping
df.pivot_table(rows=, cols=, values=) // list -> matrix
pd.melt(matrix, ids_vars=, value_vars=, var_name) // matrix -> list
pd.concat([d1, d2])
  * concat DataFrames (Series)
  * column order can be different
  * columns can be missing: filled by NaN



# Convert to numpy array / structured array
t = df.values // unstructured
t = df.to_records() //structured
  * encodes str as 'O', i.e. numpy object
  * not supported by hdf5

# Iterating
for index, row in df.iterrows():
  print row[colindex]
for col in df.iteritems():
  print col[rowindex]
df.iterrows() // iterate over rows
df.iteritems()  // iterator over columns
-> return tuple iterator

# R datasets
import numpy.rpy.common as com
com.load_data('mtcars')


# Grouping
g = df.groupby(['c1', 'c2'])
g.groups  // groups as dictionary
g.groups.keys() // group names
g.get_group('g1') // get group by name
g.ngroups // number of groups
for name, value in g: // iterate over groups
g['c1', 'c2'] // select columns; return new grouped object
g
  count(), size() // # elements
  mean()
  median()
  describe()  // summary statistics
  first()
  last()
g.aggregate([fun1, fun2]) // summarize functions as single value
g.aggregate({label:fun})
g.transform(lambda x: np.mean(x)) // group specific modification; same size as group
g.filter(lambda x: np.mean(x) > 0.5)  // return only members of groups that satisfy constraint
g.apply(lambda df: pd.DataFrame({...})  // apply arbitrary function to groups

# Summary
df.head() / df.tail()
df.columns / df.index
df.describe()

# Comparing DataFrames
df.all(axis=0)
df.all().all()
import pandas.util.testing as pt
pt.assert_frame_equal(df1, df2)

# Sql
import sqlite3 as sql
import pandas.io.sql as psql
con = sql.connect()
df = psql.read_sql('SELECT * ', con, index_col=)


# DataFrame
DataFrame(columns=['c1', 'c2'], index=['r1', 'r2'], dtype=bool)
  columns=  // column labels
  index=  // row labels
DataFrame({'id':[0, 1, 2], 'value':[1, 2, 3]})  // from dict
DataFrame.from_dict(dict) // same as before
DataFrame(np.ones((n, m)), columns=)  // from numpy array
DataFrame.from_csv

## Misc
df.values // numpy array used internally
df.values.nbytes  // memory usage
df.sort('column', ascending=True, inplace=False)  // sort by column
df.sort_index(axis=0, inplace=False)  // sort by row index
df.sort_index(axis=1) // sort columns by names
df.sort_index(by='column')  // sort by column -> like sort

## selection
.loc[rowlabel, collabel]  // purely label based
.iloc[rowindex, colindex] // purely index based
.ix[rowlabel, colindex] // mixed label index
.at[rowlabel, collabel] // scalar label based
.iat[rowindex, colindex]  // scalar index based
df[a & b | c & ~d]  // boolean indexing by rows
df.ix[:, a & b] // boolean indexing by columns

## hdf5
df.to_hdf('file', 'groupname')
df = pd.read_hdf('file', 'groupname')

# Missing values
dropna(...)  // drop columns with nan
  axis=0/1
  how=any/all
  inplace=F
isnull(), notnull()

