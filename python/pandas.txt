# dtypes
df.dtypes // list dtypes
df = df.astype(np.int32)  // convert dtype of DataFrame
df['c1'] = df['c1'].astype(str) // convert dtype column

# Misc
pd.crosstab(y, x) // contingency table
pd.concat((a, b), axis=1) // concat by columns
  keys=['a', 'b'] // create multiindex
pd.merge(a, b)  // merge dataframes by common columns
  on=['c1', 'c2'] // columns to be used for merging
  how='left'  // left join -> fill with np.NaN
df.drop_duplicates()  // remove duplicate rows -> unique rows

# Excluding / selecting columns
df = pd.DataFrame(np.arange(15).reshape(5, 3), columns=list('ABC'))
sel = list('AC')
df_sel = df[sel]
df_unsel = df[[c for c in df.columns if c not in sel]]
* Alternative: df.set_index()

# IO
to_pickle('file.pkl')
from_pickle('file.pkl')
read_hdf('file', 'groupname')
read_csv(file, ...) / read_table(file, ...)
  nrows // max number of rows to be read
  sep=',' // field separator
  sep='\s+' // separate by spaces
  usecols=['a', 'b'] // select columns by name
  usecols=[0, 1, 2] // select columns by index
  header=None // no header
to_csv(file, ...)
  sep=','
  index=True  // write index

## hdf
store = HDFStore(filename)
store.keys()
store.groups()
store['df'], store.df // read DataFrame
store['df'] = df  // write DataFrame in fixed format
store.put('df', df, ...)
  format='f'  // fixed format: fast write, no query/append/delete
  format='t'  // table format: slower write, read still fast
  data_columnns=True  // for queries where('c1 > 0.2'); see below
store.select('df', columns=['c1', 'c2'])
store.select('df', where="index=['A', 'B']")
store.select_column('df', 'c1') // if data_columns=True
store.remove('df', ...) // remove table
  where='index="A"'
  where='c1 > 10'
store.close()
store.is_open
df.to_hdf('file.h5', 'name', ...)
  format='f'  // fixed format
  format='t'  // table format
pd.read_hdf('file.h5', 'name', ...)
  columns=['c1', 'c2']
  where=["columns=['c1', 'c2'] & index>10]
  start=first_row
  stop=last_row

### Query table
~, & , ~  // not, and, or
=, >, >=  // operators
columns=['c1', 'c2']
~(columns=['c1', 'c2'])
index=['A', 'B']
chromo='chr1' // select by hierarchical index
c1 > 0.2  // if store.put('df', df, data_columns=True)

### rhdf5
store.put('df', df, format='t', data_columns=True)
df <- h5read('file.h5', '/df/table')
df <- h5read('file.h5', 'df')$table


# Index
columns // column index
  d['c1']
index // row index
  d.loc['r1']

## Rows
set_index(columns, inplace=False) // use column as index
reset_index(inplace=False) // convert index to columns
sort_index(ascending=True, inplace=False)
reindex(range(1990, 2000))  // adjust to new index
  * drop rows that are not indexed by new index
  * fill with np.NaN for newly indexed rows
  * columns=  // reindex columns
  * index=  // reindex index
  * fill_value=
index.values[0] = 'new_label' // rename index; rename column/row
index.names // names

## Columns
df.columns = ['c1', 'c2'] // rename all columns
df.rename(columns={'old':'new'}, inplace=True)  // rename selected columns
df.rename(columns=lambda x: x[1:])  // apply function for renaming
del df['col'] // delete column

## Indexing
df.loc[(r1, r2), c] // row by levels (r1, r2), column c
df.loc[r1, c] // only first level
df.loc[r1].loc[r2] == df.loc[(r1, r2)]

## Creating index
i = Index(array, dtype=)
df = DataFrame(array, index=i)
df.index = i

## Multiindex / Hierarchical index
m = MultiIndex.from_arrays([array1, array2], names=[name1, name2])
m = MultiIndex.from_tuples([('chr1', 1), ('chr1', 2), ])
m = MultiIndex.from_arrays([df.c1, df.c2], names=df.columns[1, 2])  // from DataFrame
df.index = m  // set index
pd.concat((a, b), axis=1, keys=['a', 'b'])  // concat columns of a and b
i = df.columns  // multiindex for columns
i.get_level_values(k) // get column labels level k (int or str)
i.droplevel(k) // remove level k
df.columns = df.columns.droplevel(k)

## Check if index contains value
value in df.index
'a' in df.index // single-index
'a' in df.index // multi-index first level
('a', 1) in df.index  // mutli-index both levels




# Missing values
np.nan, np.NAN, np.NaN  // pandas compatible
pd.NaT  // Not a Time, for time series
isnull(np.nan) == True
isnull()
notnull()
count() // # non-missing
fillna(0)
dropna(axis=) // drop axis with at least one missing value
interpolate() // fill missing values by interpolation
count(axis=)  // count # non-null elements

# Selection, filtering
s = df.pos  // selection based on series
s > 0 // returns boolean vector
(s > 0) & (s < 5) // boolean operator: brackets; &, |, ~ instead of and, or, not
s[(s > 0) & (s < 5)]  // section series
df[(df.pos > 0) & (df.pos < 5)] // selection data frame
s.isin([3, 5, 5]) // select single values
s.where(s > 5)  // sets entries s <= 5 to NaN
s.where(s < 5, 10)  // sets entries s <= 5 to 10
s.mask(s > 5) // sets entries s > 5 to Nan


# pandas printoptions
pd.set_option('display.width', 150) 
pd.set_option('display.height', 150)
pd.set_option('display.max_columns', 150)
pd.set_option('display.max_rows', 150)

# Melting and reshaping
df.pivot_table(rows=, cols=, values=) // list -> matrix
pd.melt(matrix, ...) // matrix -> list
  ids_vars= // key columns  (kept)
  value_vars= // value columns
  var_name= // name of new key columns
  value_name= // name of new value column
pd.concat([d1, d2])
  * concat DataFrames (Series)
  * column order can be different
  * columns can be missing: filled by NaN



# Convert to numpy array / structured array
t = df.values // unstructured
t = df.to_records() //structured
  * encodes str as 'O', i.e. numpy object
  * not supported by hdf5

# Iterating
for index, row in df.iterrows():
  print row[colindex]
for col in df.iteritems():
  print col[rowindex]
df.iterrows() // iterate over rows
df.iteritems()  // iterator over columns
-> return tuple iterator

# R datasets
import numpy.rpy.common as com
com.load_data('mtcars')


# Grouping
g = df.groupby(['c1', 'c2'])
g.groups  // groups as dictionary
g.groups.keys() // group names
g.get_group('g1') // get group by name
g.ngroups // number of groups
for name, value in g: // iterate over groups
g['c1', 'c2'] // select columns; return new grouped object
g
  count(), size() // # elements
  mean()
  median()
  describe()  // summary statistics
  first()
  last()
g.apply(lambda df: pd.DataFrame({...})  // apply arbitrary function to groups
  .apply(lambda df: df.field.iloc[0]) // return value of field of first row
  .apply(lambda df: pd.DataFrame({'f1':[df.f1.min()]}) // return min of f1
g.transform(lambda x: np.mean(x)) // group specific modification; same size as group
g.filter(lambda x: np.mean(x) > 0.5)  // return only members of groups that satisfy constraint
g.aggregate(fun)  // apply fun to each columns; return same columns (<> apply)
  .agg(fun) // same as aggregate
  .agg(np.min)  // min of each column
  .agg(lambda x: x.iloc[0]) // first entry of each column

# Summary
df.head() / df.tail()
df.columns / df.index
df.describe()

# Comparing DataFrames
df.all(axis=0)
df.all().all()
import pandas.util.testing as pt
pt.assert_frame_equal(df1, df2)

# Sql
import sqlite3 as sql
import pandas.io.sql as psql
con = sql.connect()
df = psql.read_sql('SELECT * ', con, index_col=)


# DataFrame
DataFrame(columns=['c1', 'c2'], index=['r1', 'r2'], dtype=bool)
  columns=  // column labels
  index=  // row labels
DataFrame({'id':[0, 1, 2], 'value':[1, 2, 3]})  // from dict
DataFrame.from_dict(dict) // same as before
DataFrame(np.ones((n, m)), columns=)  // from numpy array
DataFrame.from_csv

## Misc
df.values // numpy array used internally
df.values.nbytes  // memory usage
df.sort('column', ascending=True, inplace=False)  // sort by column
df.sort_index(axis=0, inplace=False)  // sort by row index
df.sort_index(axis=1) // sort columns by names
df.sort_index(by='column')  // sort by column -> like sort
df.drop(['r1', 'r2'], axis=0) // delete rows
df.drop(['c1', 'c2'], axis=1) // delete columns

## selection
.loc[rowlabel, collabel]  // purely label based
.iloc[rowindex, colindex] // purely index based
.ix[rowlabel, colindex] // mixed label index
.at[rowlabel, collabel] // scalar label based
.iat[rowindex, colindex]  // scalar index based
df[a & b | c & ~d]  // boolean indexing by rows
df.ix[:, a & b] // boolean indexing by columns

## hdf5
df.to_hdf('file', 'groupname')
df = pd.read_hdf('file', 'groupname')

# Missing values
dropna(...)  // drop columns with nan
  axis=0/1
  how=any/all
  inplace=F
isnull(), notnull()


# Series
s = Series([1, 2, 3], index=['a', 'b', 'c'])
s['a'], s.a
s.index = [0.1, 0.2, 0.3] // index does not need to be str
s[0.1]  // 1
for key, value in s.items():
  print(key, value)
