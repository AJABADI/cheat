# Operations
ops.Conv(x, nb_filter, [size, size], scope='conv1')
ops.MaxPool(x, [size, size], scope='pool1')
  stide=2
  padding='VALID'
ops.Flatten(x, scope='flatten')
opts.FC(x, nb_output, 'fc1')
ops.Dropout(x, 0.5, scope='dropout1)
ops.RepeatOp(x, op, nb_repeat, op_params, scope='conv1')
ops.BatchNorm(x)

# Scopes
with slim.scopes.arg_scope([Conv, FC], stddev=0.01, weightdecay=0.01):
  * call following operations with same args

# Training
optimizer = tf.train.AdamOptimizer()
loss = losses.CrossEntropyLoss(logits, labels)
trainer = learning.CreateTrainTensor(loss, optimizer)
learning.train(graph, train, logdir, ...)
  * SGD, saves models
  logdir // log-file / checkpoint dir
  number_of_steps=x // number of gradient steps
  save_summaries=secs=x // save summaries every x secs
  save_interval_secs=x // save checkpoints every x secs

# Variables
variables.get_all_variables() // all defined variables
variables.get_variables_by_names(...)
  weights, biases 

# Metrics
metrics.

